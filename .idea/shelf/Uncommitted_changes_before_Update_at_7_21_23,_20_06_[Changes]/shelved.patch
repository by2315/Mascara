Index: scripts/amz_v2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import sys\nfrom datetime import datetime\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait as wait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException\nfrom selenium.webdriver.chrome.options import Options\nimport jsonpickle\nimport difflib\n\nproduct_name_1 = \"Covergirl Lash Blast Volume Mascara, Very Black\"\nproduct_name_2 = \"Covergirl Lash Blast Volume Waterproof Mascara, Volumizing Mascara, Black, Pack of 2\"\n\nmatches = difflib.get_close_matches(product_name_1, [product_name_2], n=1, cutoff=0.8)\n\nif matches:\n    print(\"The product names are considered similar.\")\nelse:\n    print(\"The product names are not considered similar.\")\n\n\ndef product_exists(product_name, products_dataset):\n    existing_products = [product['product_name'] for product in products_dataset]\n    matches = difflib.get_close_matches(product_name, existing_products, n=1, cutoff=0.8)\n    return True if matches else False\n\ndef webdriver_options():\n    chrome_options = Options()\n    #chrome_options.add_argument('--headless')\n    chrome_options.add_argument('--disable-popup-blocking')\n    chrome_options.add_argument('--disable-new-window')\n    return chrome_options\ndef search_amazon(search_product_name):\n\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options())\n    driver.get('https://www.amazon.com')\n\n    #Search Product Name in Search Bar of Amazon\n    try:\n        search = driver.find_element(By.ID, 'twotabsearchtextbox')\n        search.send_keys(search_product_name)\n        search_button = driver.find_element(By.ID, 'nav-search-submit-button')\n        search_button.click()\n    except NoSuchElementException:\n        try:\n            driver.refresh()\n            search = driver.find_element(By.ID, 'nav-bb-search')\n            search.send_keys(search_product_name)\n            search_button = driver.find_element(By.CLASS_NAME, 'nav-bb-button')\n            search_button.click()\n        except NoSuchElementException:\n            print(\"No Search Bar, Couldn't search because didn't find element 'twotabsearchtextbox' or 'nav-bb-search'!\")\n            sys.exit()\n\n    #Index through each product in search result and scrap product data\n    products = []\n\n    last_product_page = 0\n    while last_product_page != 1:\n        try:\n            pagination = driver.find_element(By.XPATH,\n                                             '//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]').get_attribute(\n                'href')\n            products.append(scrape_page(driver, products))\n\n            #Save Products Dataset\n            save_products(keyword, products)\n\n            # Open the Next Product Search Page\n            last_product_page = 0\n            driver.get(pagination)\n\n        except NoSuchElementException:\n            last_product_page = 1\n            scrape_page(driver, products)\n\n            # Save Products Dataset\n            save_products(keyword, products)\n\n    driver.quit()\n    return products\ndef scrape_page(web_driver,products_dataset):\n    try:\n        items = wait(web_driver, 30).until(\n            EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, \"s-result-item s-asin\")]')))\n    except NoSuchElementException:\n        web_driver.refresh()\n        try:\n            items = wait(web_driver, 30).until(\n                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, \"s-result-item s-asin\")]')))\n        except NoSuchElementException:\n            print('No products found!')\n            return products_dataset\n    for item in items:\n        product = scrap_product(item)\n        products_dataset.append(product)\n\n    return products_dataset\ndef scrap_product(individual_item):\n    product = {}\n    # Extract Product Name\n    try:\n        name = individual_item.find_element(By.XPATH, '//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]')\n        product['name'] = name.text\n    except NoSuchElementException:\n        product['name'] = []\n        pass\n\n    # Extract Product Asin\n    try:\n        product_asin = individual_item.get_attribute(\"data-asin\")\n        product['asin'] = product_asin\n    except NoSuchElementException:\n        product['asin'] = []\n        pass\n\n    # Extract Product Price\n    try:\n        whole_price = individual_item.find_elements(By.XPATH, './/span[@class=\"a-price-whole\"]')\n        fraction_price = individual_item.find_elements(By.XPATH, './/span[@class=\"a-price-fraction\"]')\n\n        if whole_price != [] and fraction_price != []:\n            price = '.'.join([whole_price[0].text, fraction_price[0].text])\n        else:\n            price = 0\n        product['price'] = price\n    except NoSuchElementException:\n        product['price'] = []\n\n    # Extract Rating\n    try:\n        rating_box = individual_item.find_elements(By.XPATH, './/div[@class=\"a-row a-size-small\"]/span')\n\n        if rating_box:\n            rating = rating_box[0].get_attribute('aria-label')\n            rating_num = rating_box[1].get_attribute('aria-label')\n        else:\n            rating, rating_num = 0, 0\n\n        product['rating'] = rating\n        product['rating_Num'] = rating_num\n    except NoSuchElementException:\n        product['rating'] = []\n        product['rating_Num'] = []\n\n    # Find Product URL and Open URL\n    product_url = individual_item.find_element(By.XPATH,\n                                    './/a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]').get_attribute(\n        \"href\")\n    product['product_url'] = product_url\n\n    driver_product = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options())\n    driver_product.get(product_url)\n\n    try:\n        wait(driver_product, 30).until(\n            EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, \"a-section a-spacing-none\")]')))\n    except (NoSuchElementException, TimeoutException) as e:\n        driver_product.refresh()\n\n    # Find and open All Reviews URL for Product\n    try:\n        review_url = driver_product.find_elements(By.XPATH,\n                                                  '//a[@data-hook=\"see-all-reviews-link-foot\" and contains(@class, \"a-link-emphasis a-text-bold\")]')\n    except (NoSuchElementException, TimeoutException) as e:\n        driver_product.refresh()\n        review_url = driver_product.find_elements(By.XPATH,\n                                                  '//a[@data-hook=\"see-all-reviews-link-foot\" and contains(@class, \"a-link-emphasis a-text-bold\")]')\n\n    if review_url != []:\n\n        ############\n        print(review_url[0].get_attribute('href'))\n        ############\n\n        product['review_url'] = review_url[0].get_attribute('href')\n        driver_product.get(product['review_url'])\n        try:\n            wait(driver_product, 30).until(\n                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, \"main\")]')))\n        except (NoSuchElementException, TimeoutException) as e:\n            try:\n                driver_product.refresh()\n                wait(driver_product, 30).until(\n                    EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, \"main\")]')))\n            except (NoSuchElementException, TimeoutException) as e:\n                return product\n\n    else:\n        product['review_url'] = []\n        product['reviews'] = []\n        return product\n\n\n    # Extract Reviews for product\n    reviews = []\n    last_review_page = 0\n\n    while last_review_page != 1:\n\n        try:\n            next_page = driver_product.find_element(By.XPATH, '//li[@class=\"a-last\"]//a')\n\n            reviews.append(scrap_product_reviews(driver_product,reviews))\n\n            driver_product.get(next_page.get_attribute('href'))\n\n            # Check if next page is loaded\n            try:\n                wait(driver_product, 30).until(\n                    EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, \"main\")]')))\n            except NoSuchElementException:\n                driver_product.refresh()\n\n            last_review_page = 0\n\n        except NoSuchElementException:\n            last_review_page = 1\n            reviews.append(scrap_product_reviews(driver_product,reviews))\n\n    product['reviews'] = reviews\n\n    # Close the driver product webdriver\n    driver_product.quit()\n\n    return product\ndef scrap_product_reviews(web_driver_product_reviews, review_dataset):\n    # Extract review elements\n\n    try:\n        review_elements = web_driver_product_reviews.find_elements(By.XPATH, '//div[@data-hook=\"review\"]')\n    except NoSuchElementException:\n        print(\"No Reviews Element Found in Page\")\n        review={}\n        review_dataset.append(review)\n        return review_dataset\n\n    for review_element in review_elements:\n        review = {}\n\n        # Extract reviewer name\n        try:\n            reviewer_element = review_element.find_element(By.XPATH, './/span[@class=\"a-profile-name\"]')\n            review['reviewer'] = reviewer_element.text.strip()\n        except NoSuchElementException:\n            review['reviewer'] = 'None'\n            pass\n\n        # Extract review rating\n        try:\n            rating_element = review_element.find_element(By.XPATH, './/i[@data-hook=\"review-star-rating\"]')\n            rating = rating_element.find_element(By.XPATH, './/span[@class=\"a-icon-alt\"]')\n            review['rating'] = rating.get_attribute('textContent').strip().split()[0]\n        except NoSuchElementException:\n            review['rating'] = 'None'\n            pass\n\n        # Extract review text\n        try:\n            review_text_element = review_element.find_element(By.XPATH, './/span[@data-hook=\"review-body\"]')\n            review['review_text'] = review_text_element.text.strip()\n        except NoSuchElementException:\n            review['review_text'] = 'None'\n            pass\n\n        # Extract picture\n        try:\n            review_images_element = review_element.find_elements(By.XPATH,\n                                                                 './/div[@class=\"review-image-tile-section\"]//img')\n            review['img'] = [image.get_attribute('src') for image in review_images_element]\n        except NoSuchElementException:\n            review['img'] = 'None'\n            pass\n\n        #Append review to review_dataset\n        review_dataset.append(review)\n\n    return review_dataset\ndef save_products(product_name, products_dataset):\n    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n    file_name = f'{product_name}_amazon_reviews_{current_time}.txt'\n\n    with open(file_name, 'w') as f:\n        serialized_data = jsonpickle.encode(products_dataset)\n        f.write(serialized_data)\n\nkeyword = \"Mascara\"\nsearch_amazon(keyword)\n\n# Problems\n# Convert downloaded image url to 64bit data (ChatGPT)\n# Add Checking Mechanism to compare newly added product to existing product\n\n\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/amz_v2.py b/scripts/amz_v2.py
--- a/scripts/amz_v2.py	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
+++ b/scripts/amz_v2.py	(date 1689983683661)
@@ -9,23 +9,6 @@
 from selenium.common.exceptions import NoSuchElementException, TimeoutException
 from selenium.webdriver.chrome.options import Options
 import jsonpickle
-import difflib
-
-product_name_1 = "Covergirl Lash Blast Volume Mascara, Very Black"
-product_name_2 = "Covergirl Lash Blast Volume Waterproof Mascara, Volumizing Mascara, Black, Pack of 2"
-
-matches = difflib.get_close_matches(product_name_1, [product_name_2], n=1, cutoff=0.8)
-
-if matches:
-    print("The product names are considered similar.")
-else:
-    print("The product names are not considered similar.")
-
-
-def product_exists(product_name, products_dataset):
-    existing_products = [product['product_name'] for product in products_dataset]
-    matches = difflib.get_close_matches(product_name, existing_products, n=1, cutoff=0.8)
-    return True if matches else False
 
 def webdriver_options():
     chrome_options = Options()
@@ -81,6 +64,7 @@
             save_products(keyword, products)
 
     driver.quit()
+    print("All Amazon Reviews Scrapped for Product!")
     return products
 def scrape_page(web_driver,products_dataset):
     try:
@@ -288,11 +272,14 @@
         f.write(serialized_data)
 
 keyword = "Mascara"
-search_amazon(keyword)
+keyword_products=search_amazon(keyword)
+save_products(keyword,keyword_products)
 
 # Problems
 # Convert downloaded image url to 64bit data (ChatGPT)
 # Add Checking Mechanism to compare newly added product to existing product
+# Add Amazon verification using twocaptcha
+# Add hide my ip to chrome
 
 
 
Index: scripts/amz.py
===================================================================
diff --git a/scripts/amz.py b/scripts/amz.py
deleted file mode 100644
--- a/scripts/amz.py	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
+++ /dev/null	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
@@ -1,295 +0,0 @@
-import requests
-from bs4 import BeautifulSoup
-import time
-import json
-
-'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.366'
-
-def get_total_pages(search_query):
-    # Format the search query for the Amazon URL
-    formatted_query = search_query.replace(' ', '+')
-    url = f'https://www.amazon.com/s?k={formatted_query}'
-
-    # Define user agent header
-    headers = ({
-        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
-        'Accept-Language': 'en-US, en;q=0.5'
-    })
-
-    # Send a GET request to the search results page
-    response = requests.get(url, headers=headers)
-    if response.status_code == 200:
-        soup = BeautifulSoup(response.content, 'html.parser')
-
-        # Find the pagination container
-        pagination = soup.find('span', {'class': 's-pagination-item s-pagination-disabled'})
-        if pagination:
-            total_pages = int(pagination.text.strip())
-            return total_pages
-
-    return 0
-
-## Need to Optimize out the Sponsored and Highly rated reviews because they will be duplicates
-def search_amazon_mascara(search_query, num_pages):
-    search_results = []
-
-    total_pages = get_total_pages(search_query)
-    if total_pages == 0:
-        print("No search results found.")
-
-
-    # Format the search query for the Amazon URL
-    formatted_query = search_query.replace(' ', '+')
-
-    for page in range(1, total_pages + 1):
-
-        '#################'
-        page=1
-        '#################'
-
-        print(page, 'Out of', total_pages)
-
-        url = f'https://www.amazon.com/s?k={formatted_query}&page={page}'
-
-        # Define user agent header
-        headers = ({
-            'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 10066.0.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
-            'Accept-Language': 'en-US, en;q=0.5'
-
-        })
-
-        # Send a GET request to the search results page
-        response = requests.get(url, headers=headers)
-
-        if response.status_code == 200:
-            soup = BeautifulSoup(response.content, 'html.parser')
-
-            # Find the search result containers
-            result_containers = soup.find_all('div', {'data-component-type': 's-search-result'})
-
-            for container in result_containers:
-                result = {}
-
-                # Extract the product name
-                '####################'
-                product_name = result_containers[0].find('span', {'class': 'a-size-base-plus'})
-                '####################'
-
-                if product_name:
-                    result['product_name'] = product_name.text.strip()
-                else:
-                    continue
-
-                # Extract the product URL
-                '####################'
-                product_url = result_containers[0].find('a', {'class': 'a-link-normal'})
-                '####################'
-
-                if product_url:
-                    result['product_url'] = 'https://www.amazon.com' + product_url['href']
-                else:
-                    continue
-
-                # Extract the Review URL
-                product_response = requests.get(result['product_url'], headers=headers)
-
-                if product_response.status_code == 200:
-                    product_soup = BeautifulSoup(product_response.content, 'html.parser')
-                    review_url = product_soup.find('a', {'data-hook': 'see-all-reviews-link-foot'})
-
-                    if review_url:
-                        result['review_url'] = 'https://www.amazon.com' + review_url['href']
-                    else:
-                        continue
-
-                result['reviews']=scrape_amazon_reviews(result['review_url'])
-
-                search_results.append(result)
-
-        # Add a delay of 2 seconds between requests to avoid overwhelming the server
-        time.sleep(2)
-    return search_results
-
-#Need to Optimize to find all reviews
-def scrape_amazon_reviews(review_url):
-
-    reviews = []
-
-    # Define user agent header
-    review_headers = {
-        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:70.0) Gecko/20100101 Firefox/70.0'
-    }
-
-    for page_number in range(1, 1000):
-
-        # Send a GET request to the product page
-        review_response = requests.get(result['review_url'], headers=review_headers)
-
-        print(review_response)
-
-        if review_response.status_code == 200:
-
-            review_soup = BeautifulSoup(review_response.text, 'html.parser')
-
-            # Find the review containers
-
-            review_containers = review_soup.find_all('div', {'data-hook':'review'})
-
-            print(review_containers)
-
-            if not (review_containers):
-                return reviews
-
-            for container in review_containers:
-                review = {}
-
-                # Extract the reviewer name
-                reviewer = container.find('span', {'class': 'a-profile-name'})
-                if reviewer:
-                    review['reviewer'] = reviewer.text.strip()
-                else:
-                    review['reviewer'] = 'Anonymous'
-
-                # Extract the review rating
-                rating = container.find('span', {'class': 'a-icon-alt'})
-                if rating:
-                    review['rating'] = rating.text.strip().split()[0]
-                else:
-                    review['rating'] = 'Not available'
-
-                # Extract the review text
-                review_text = container.find('span', {'data-hook': 'review-body'})
-                if review_text:
-                    review['review_text'] = review_text.text.strip()
-                else:
-                    review['review_text'] = 'No review text available'
-
-                reviews.append(review)
-
-
-search_query = 'mascara'
-num_pages =1
-# Perform the search and get the search results
-search_results = search_amazon_mascara(search_query, num_pages)
-
-filename = 'amz_review_URL.txt'
-with open(filename,'w')as f:
-    f.write(json.dumps(search_results))
-
-# Iterate over each search result
-for result in search_results:
-    product_name = search_results['product_name']
-    product_url = search_results['product_url']
-
-    print(f'Product Name: {product_name}')
-    print(f'Product URL: {product_url}')
-
-    # Scrape the reviews for the product
-    scraped_reviews = scrape_amazon_reviews(product_url)
-
-    if scraped_reviews:
-        for review in scraped_reviews:
-            print(f'Reviewer: {review["reviewer"]}')
-            print(f'Rating: {review["rating"]}')
-            print(f'Review: {review["review_text"]}')
-            print('---')
-
-# Save the reviews to a text file
-filename = 'amazon_reviews.txt'
-save_reviews_to_file(scraped_reviews, filename)
-print(f'Reviews saved to file: {filename}')
-
-https://www.amazon.com/Covergirl-Lash-Blast-Mascara-Black/product-reviews/B00EMAM9BC/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&pageNumber=1
-https://www.amazon.com/Covergirl-Lash-Blast-Mascara-Black/product-reviews/B00EMAM9BC/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2
-https://www.amazon.com/Covergirl-Lash-Blast-Mascara-Black/product-reviews/B00EMAM9BC/ref=cm_cr_getr_d_paging_btm_next_3?ie=UTF8&reviewerType=all_reviews&pageNumber=3
-
-'---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
-from selenium import webdriver
-'from selenium.webdriver.firefox.service import Service
-from selenium.webdriver.chrome.service import Service
-from webdriver_manager.chrome import ChromeDriverManager
-'from webdriver_manager.firefox import GeckoDriverManager
-from selenium.webdriver.common.proxy import Proxy, ProxyType
-
-HOSTNAME = 'us.smartproxy.com'
-PORT = '10000'
-DRIVER = 'CHROME'
-
-def smartproxy():
-  prox = Proxy()
-  prox.proxy_type = ProxyType.MANUAL
-  prox.http_proxy = '{hostname}:{port}'.format(hostname = HOSTNAME, port = PORT)
-  prox.ssl_proxy = '{hostname}:{port}'.format(hostname = HOSTNAME, port = PORT)
-  if DRIVER == 'FIREFOX':
-    capabilities = webdriver.DesiredCapabilities.FIREFOX
-  elif DRIVER == 'CHROME':
-    capabilities = webdriver.DesiredCapabilities.CHROME
-  prox.add_to_capabilities(capabilities)
-  return capabilities
-
-def webdriver_example():
-  if DRIVER == 'FIREFOX':
-    browser = webdriver.Firefox(service=Service(GeckoDriverManager().install()), proxy=smartproxy())
-  elif DRIVER == 'CHROME':
-    browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()), desired_capabilities=smartproxy())
-
-  browser.get('https://www.amazon.com/Covergirl-Lash-Blast-Mascara-Black/product-reviews/B00EMAM9BC/ref=cm_cr_dp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=4')
-
-  print(browser.page_source)
-  browser.quit()
-
-if __name__ == '__main__':
-  webdriver_example()
-
-'---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
-  from selenium import webdriver
-  from selenium.webdriver.common.by import By
-  from selenium.webdriver.support.ui import WebDriverWait
-  from selenium.webdriver.support import expected_conditions as EC
-
-
-  def scrape_amazon_reviews(url):
-      # Set up Selenium webdriver
-      driver = webdriver.Chrome()
-      driver.get(url)
-
-      # Wait for the review section to load
-      WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'cm_cr-review_list')))
-
-      # Extract the review elements
-      review_elements = driver.find_elements(By.XPATH, '//div[@data-hook="review"]')
-
-      products = []
-      for review_element in review_elements:
-          product = {}
-
-          # Extract reviewer name
-          reviewer_element = review_element.find_element(By.XPATH, './/span[@class="a-profile-name"]')
-          review['reviewer'] = reviewer_element.text.strip()
-
-          # Extract review rating
-          rating_element = review_element.find_element(By.XPATH, './/i[@data-hook="review-star-rating"]')
-          review['rating'] = rating_element.get_attribute('aria-label').strip().split()[0]
-
-          # Extract review text
-          review_text_element = review_element.find_element(By.XPATH, './/span[@data-hook="review-body"]')
-          review['review_text'] = review_text_element.text.strip()
-
-          reviews.append(review)
-
-      # Close the Selenium webdriver
-      driver.quit()
-
-      return reviews
-
-
-  # Example usage
-  url = 'https://www.amazon.com/product-reviews/PRODUCT_ID'
-  reviews = scrape_amazon_reviews(url)
-
-  for review in reviews:
-      print(f'Reviewer: {review["reviewer"]}')
-      print(f'Rating: {review["rating"]}')
-      print(f'Review: {review["review_text"]}')
-      print('---')
-
Index: scripts/Sephora.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/Sephora.py b/scripts/Sephora.py
deleted file mode 100644
--- a/scripts/Sephora.py	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
+++ /dev/null	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
@@ -1,10 +0,0 @@
-import sys
-from selenium import webdriver
-from selenium.webdriver.chrome.service import Service
-from webdriver_manager.chrome import ChromeDriverManager
-from selenium.webdriver.common.by import By
-from selenium.webdriver.support.ui import WebDriverWait as wait
-from selenium.webdriver.support import expected_conditions as EC
-from selenium.common.exceptions import NoSuchElementException
-from selenium.webdriver.chrome.options import Options
-
Index: scripts/amz_v2_ori.py
===================================================================
diff --git a/scripts/amz_v2_ori.py b/scripts/amz_v2_ori.py
deleted file mode 100644
--- a/scripts/amz_v2_ori.py	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
+++ /dev/null	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
@@ -1,298 +0,0 @@
-import json
-import os
-import sys
-from datetime import datetime
-from selenium import webdriver
-from selenium.webdriver.chrome.service import Service
-from webdriver_manager.chrome import ChromeDriverManager
-from selenium.webdriver.common.by import By
-from selenium.webdriver.support.ui import WebDriverWait as wait
-from selenium.webdriver.support import expected_conditions as EC
-from selenium.common.exceptions import NoSuchElementException
-from selenium.webdriver.chrome.options import Options
-
-def webdriver_options():
-    chrome_options = Options()
-    # options.add_argument('--headless')
-    chrome_options.add_argument('--disable-popup-blocking')
-    chrome_options.add_argument('--disable-new-window')
-    return chrome_options
-
-def search_amazon(keyword,driver):
-    try:
-        search = driver.find_element(By.ID, 'twotabsearchtextbox')
-        search.send_keys(keyword)
-        search_button = driver.find_element(By.ID, 'nav-search-submit-button')
-        search_button.click()
-        return  (driver)
-    except NoSuchElementException:
-        try:
-            search = driver.find_element(By.ID, 'nav-bb-search')
-            search.send_keys(keyword)
-            search_button = driver.find_element(By.CLASS_NAME, 'nav-bb-button')
-            search_button.click()
-        except NoSuchElementException:
-            print("No Search Bar, Couldn't search because didn't find element 'twotabsearchtextbox' or 'nav-bb-search'!")
-            sys.exit()
-
-def scrape_page(driver,products):
-    try:
-        items = wait(driver, 10).until(
-            EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "s-result-item s-asin")]')))
-    except NoSuchElementException:
-        driver.refresh()
-        try:
-            items = wait(driver, 10).until(
-                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "s-result-item s-asin")]')))
-        except NoSuchElementException:
-            print('No products found!')
-            sys.exit()
-    for item in items:
-        product = {}
-
-        # Extract Product Name
-        try:
-            name = item.find_element(By.XPATH, '//span[@class="a-size-base-plus a-color-base a-text-normal"]')
-            product['name'] = name.text
-        except NoSuchElementException:
-            product['name'] = []
-            pass
-
-        # Extract Product Asin
-        try:
-            product_asin = item.get_attribute("data-asin")
-            product['asin'] = product_asin
-        except NoSuchElementException:
-            product['asin'] = []
-            pass
-
-        # Extract Product Price
-        try:
-            whole_price = item.find_elements(By.XPATH, './/span[@class="a-price-whole"]')
-            fraction_price = item.find_elements(By.XPATH, './/span[@class="a-price-fraction"]')
-
-            if whole_price != [] and fraction_price != []:
-                price = '.'.join([whole_price[0].text, fraction_price[0].text])
-            else:
-                price = 0
-            product['price'] = price
-        except NoSuchElementException:
-            product['price'] = []
-
-        # Extract Rating
-        try:
-            rating_box = item.find_elements(By.XPATH, './/div[@class="a-row a-size-small"]/span')
-
-            if rating_box:
-                rating = rating_box[0].get_attribute('aria-label')
-                rating_num = rating_box[1].get_attribute('aria-label')
-            else:
-                ratings, ratings_num = 0, 0
-
-            product['rating'] = rating
-            product['rating_Num'] = rating_num
-        except NoSuchElementException:
-            product['rating'] = []
-            product['rating_Num'] = []
-
-        # find link
-
-        product_url = item.find_element(By.XPATH,
-                                        './/a[@class="a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal"]').get_attribute(
-            "href")
-        product['product_url'] = product_url
-
-        # Find and open All Reviews URL for Product
-
-        driver_product = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options())
-        driver_product.get(product_url)
-
-        try:
-            wait(driver_product , 30).until(
-                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "a-section a-spacing-none")]')))
-        except NoSuchElementException:
-            driver_product.refresh()
-
-        try:
-            review_url = driver_product.find_elements(By.XPATH,'//a[@data-hook="see-all-reviews-link-foot" and contains(@class, "a-link-emphasis a-text-bold")]')
-        except NoSuchElementException:
-            driver_product.refresh()
-            review_url = driver_product.find_elements(By.XPATH,'//a[@data-hook="see-all-reviews-link-foot" and contains(@class, "a-link-emphasis a-text-bold")]')
-
-        if review_url:
-            product['review_url'] = review_url[0].get_attribute('href')
-        else:
-            product['review_url'] = "none"
-
-        driver_product.get(product['review_url'])
-
-        try:
-            wait(driver_product, 10).until(
-                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, "main")]')))
-
-        except NoSuchElementException:
-            driver_product.refresh()
-
-        # Extract Reviews for product
-
-        reviews = []
-        last_review_page = 0
-
-        while last_review_page != 1:
-
-            try:
-                next_page = driver_product.find_element(By.XPATH, '//li[@class="a-last"]//a')
-
-                #Extract review elements
-                try:
-                    review_elements = driver_product.find_elements(By.XPATH, '//div[@data-hook="review"]')
-                except NoSuchElementException:
-                    print("No Reviews Found in Page")
-                    break
-
-                for review_element in review_elements:
-                    review = {}
-
-                    # Extract reviewer name
-                    try:
-                        reviewer_element = review_element.find_element(By.XPATH, './/span[@class="a-profile-name"]')
-                        review['reviewer'] = reviewer_element.text.strip()
-                    except NoSuchElementException:
-                        review['reviewer'] = 'None'
-                        pass
-
-                    # Extract review rating
-                    try:
-                        rating_element = review_element.find_element(By.XPATH, './/i[@data-hook="review-star-rating"]')
-                        rating = rating_element.find_element(By.XPATH, './/span[@class="a-icon-alt"]')
-                        review['rating'] = rating.get_attribute('textContent').strip().split()[0]
-                    except NoSuchElementException:
-                        review['rating'] = 'None'
-                        pass
-
-                    # Extract review text
-                    try:
-                        review_text_element = review_element.find_element(By.XPATH, './/span[@data-hook="review-body"]')
-                        review['review_text'] = review_text_element.text.strip()
-                    except NoSuchElementException:
-                        review['review_text'] = 'None'
-                        pass
-
-                    # Extract picture
-                    try:
-                        review_images_element = review_element.find_elements(By.XPATH,
-                                                                             './/div[@class="review-image-tile-section"]//img')
-                        review_images_url = [image.get_attribute('src') for image in review_images_element]
-                        review['img'] = review_images_url
-                    except NoSuchElementException:
-                        review['img'] = 'None'
-                        pass
-
-                    reviews.append(review)
-
-                next_page = driver_product.find_element(By.XPATH, '//li[@class="a-last"]//a')
-                driver_product.get(next_page.get_attribute('href'))
-
-                #Check if next page is loaded
-                try:
-                    wait(driver_product, 10).until(
-                        EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, "main")]')))
-                except NoSuchElementException:
-                    driver_product.refresh()
-
-                last_review_page = 0
-
-            except NoSuchElementException:
-                last_review_page = 1
-                review_elements = driver_product.find_elements(By.XPATH, '//div[@data-hook="review"]')
-
-                for review_element in review_elements:
-                    review = {}
-
-                    # Extract reviewer name
-                    try:
-                        reviewer_element = review_element.find_element(By.XPATH, './/span[@class="a-profile-name"]')
-                        review['reviewer'] = reviewer_element.text.strip()
-                    except NoSuchElementException:
-                        review['reviewer'] = 'None'
-                        pass
-
-                    # Extract review rating
-                    try:
-                        rating_element = review_element.find_element(By.XPATH, './/i[@data-hook="review-star-rating"]')
-                        rating = rating_element.find_element(By.XPATH, './/span[@class="a-icon-alt"]')
-                        review['rating'] = rating.get_attribute('textContent').strip().split()[0]
-                    except NoSuchElementException:
-                        review['rating'] = 'None'
-                        pass
-
-                    # Extract review text
-                    try:
-                        review_text_element = review_element.find_element(By.XPATH, './/span[@data-hook="review-body"]')
-                        review['review_text'] = review_text_element.text.strip()
-                    except NoSuchElementException:
-                        review['review_text'] = 'None'
-                        pass
-
-                    # Extract picture
-                    try:
-                        review_images_element = review_element.find_elements(By.XPATH,
-                                                                             './/div[@class="review-image-tile-section"]//img')
-                        review_images_url = [image.get_attribute('src') for image in review_images_element]
-                        review['img'] = review_images_url
-                    except NoSuchElementException:
-                        review['img'] = 'None'
-                        pass
-                    reviews.append(review)
-
-            product['reviews'] = reviews
-
-        # Close the driver product webdriver
-        driver_product.quit()
-        products.append(product)
-    return products
-
-# Find IP Address used by Webdriver in with this website (https://api.ipify.org?format=json)
-
-driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options())
-driver.get('https://www.amazon.com')
-products = []
-keyword = "Mascara"
-search_amazon(keyword,driver)
-scrape_page(driver,products)
-
-last_product_page = 0
-while last_product_page != 1:
-    try:
-        pagination = driver.find_element(By.XPATH,
-                                         '//a[@class="s-pagination-item s-pagination-next s-pagination-button s-pagination-separator"]').get_attribute(
-            'href')
-        scrape_page(driver,products)
-
-        # Open the Next Product Search Page
-        last_product_page = 0
-        pagination = driver.find_element(By.XPATH,
-                                         '//a[@class="s-pagination-item s-pagination-next s-pagination-button s-pagination-separator"]').get_attribute(
-            'href')
-        driver.get(pagination)
-
-    except NoSuchElementException:
-        last_product_page = 1
-        scrape_page(driver, products)
-
-driver.quit()
-
-timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
-
-filename = f'{keyword}_amazon_reviews_{timestamp}.txt'
-
-filepath = os.path.join('Users/bill/Desktop/Know Project/scrap_data', filename)
-
-with open(filename,'w')as f:
-    f.write(json.dumps(products))
-
-# Problems
-# Convert downloaded image url to 64bit data (ChatGPT)
-
-
-
Index: scripts/sephora.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/sephora.py b/scripts/sephora.py
new file mode 100644
--- /dev/null	(date 1689909355150)
+++ b/scripts/sephora.py	(date 1689909355150)
@@ -0,0 +1,333 @@
+import sys
+from datetime import datetime
+from selenium import webdriver
+from selenium.webdriver.chrome.service import Service
+from webdriver_manager.chrome import ChromeDriverManager
+from selenium.webdriver.common.by import By
+from selenium.webdriver.support.ui import WebDriverWait as wait
+from selenium.webdriver.support import expected_conditions as EC
+from selenium.common.exceptions import NoSuchElementException, TimeoutException
+from selenium.webdriver.chrome.options import Options
+import jsonpickle
+#import difflib
+#from twocaptcha import TwoCaptcha
+
+keyword = "mascara"
+
+# def amazon_verification(web_driver):
+#     while try_times != 0:
+#         try_times -= 1
+#         # take screen shoot of whole screen and crop the captcha out
+#         captcha_input.clear()
+#         change_captcha = web_driver.find_element(By.XPATH, "//a[@class='logoImg']")
+#         change_captcha.click()
+#         time.sleep(5)
+#         web_driver.save_screenshot('captcha.png')
+#         screenshot = Image.open('captcha.png')
+#         screenshot = screenshot.crop((970, 645, 1085, 685))
+#         screenshot.save('captcha.png')
+#
+#         solver = TwoCaptcha('e0bdd73c7d0c92e92e3d4343c63686c8')
+#
+#         result = solver.normal('captcha.png')['code']
+#         print(result)
+#
+#         # send key to log_in
+#         captcha_input.send_keys(result)
+#
+#         register_submit.click()
+#         time.sleep(20)
+#         browser.get("https://www.moonbbs.com/")
+#         try:
+#             browser.find_element(By.XPATH, './/*[@class="login_btnb"]')
+#             print('Register failed, remaining times: ', try_times)
+#         except:
+#             # check if we Log_in sucessfuly
+#             print('Register success')
+#             try_times = 0
+#     time.sleep(100)
+
+def webdriver_options():
+    chrome_options = Options()
+    #chrome_options.add_argument('--headless')
+    chrome_options.add_argument('--disable-popup-blocking')
+    chrome_options.add_argument('--disable-new-window')
+    return chrome_options
+def search_amazon(search_product_name):
+
+    search_url = "https://www.sephora.com/search?keyword=" + keyword
+
+    driver = webdriver.Chrome(service=Service(ChromeDriverManager(version="114.0.5735.90").install()), options=webdriver_options())
+    driver.get(search_url)
+
+    try:
+        wait(driver, 30).until(
+            EC.presence_of_all_elements_located((By.XPATH, '//a[contains(@aria-label, "Sephora Homepage")]')))
+    except (NoSuchElementException, TimeoutException) as e:
+        driver.refresh()
+        try:
+            driver.refresh()
+            wait(driver, 30).until(
+                EC.presence_of_all_elements_located((By.XPATH, '//a[contains(@aria-label, "Sephora Homepage")]')))
+        except (NoSuchElementException, TimeoutException) as e:
+            print("no main found")
+            sys.exit()
+
+    #Click 'Show Products' until all products show
+
+    find_element(By.XPATH, '//span[@class="a-size-base-plus a-color-base a-text-normal"]')
+    try:
+        wait(driver, 30).until(
+            EC.presence_of_all_elements_located((By.XPATH, '//a[contains(@aria-label, "Sephora Homepage")]')))
+    except (NoSuchElementException, TimeoutException) as e:
+        driver.refresh()
+        try:
+            driver.refresh()
+            wait(driver, 30).until(
+                EC.presence_of_all_elements_located((By.XPATH, '//a[contains(@aria-label, "Sephora Homepage")]')))
+        except (NoSuchElementException, TimeoutException) as e:
+            print("no main found")
+
+    #Index through each product in search result and scrap product data
+    products = []
+
+    last_product_page = 0
+    while last_product_page != 1:
+        try:
+            pagination = driver.find_element(By.XPATH,
+                                             '//a[@class="s-pagination-item s-pagination-next s-pagination-button s-pagination-separator"]').get_attribute(
+                'href')
+            products.append(scrape_page(driver, products))
+
+            #Save Products Dataset
+            save_products(keyword, products)
+
+            # Open the Next Product Search Page
+            last_product_page = 0
+            driver.get(pagination)
+
+        except NoSuchElementException:
+            last_product_page = 1
+            scrape_page(driver, products)
+
+            # Save Products Dataset
+            save_products(keyword, products)
+
+    driver.quit()
+    return products
+def scrape_page(web_driver,products_dataset):
+    try:
+        items = wait(web_driver, 30).until(
+            EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "s-result-item s-asin")]')))
+    except NoSuchElementException:
+        web_driver.refresh()
+        try:
+            items = wait(web_driver, 30).until(
+                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "s-result-item s-asin")]')))
+        except NoSuchElementException:
+            print('No products found!')
+            return products_dataset
+    for item in items:
+        product = scrap_product(item)
+        products_dataset.append(product)
+
+    return products_dataset
+def scrap_product(individual_item):
+    product = {}
+    # Extract Product Name
+    try:
+        name = individual_item.find_element(By.XPATH, '//span[@class="a-size-base-plus a-color-base a-text-normal"]')
+        product['name'] = name.text
+    except NoSuchElementException:
+        product['name'] = []
+        pass
+
+    # Extract Product Asin
+    try:
+        product_asin = individual_item.get_attribute("data-asin")
+        product['asin'] = product_asin
+    except NoSuchElementException:
+        product['asin'] = []
+        pass
+
+    # Extract Product Price
+    try:
+        whole_price = individual_item.find_elements(By.XPATH, './/span[@class="a-price-whole"]')
+        fraction_price = individual_item.find_elements(By.XPATH, './/span[@class="a-price-fraction"]')
+
+        if whole_price != [] and fraction_price != []:
+            price = '.'.join([whole_price[0].text, fraction_price[0].text])
+        else:
+            price = 0
+        product['price'] = price
+    except NoSuchElementException:
+        product['price'] = []
+
+    # Extract Rating
+    try:
+        rating_box = individual_item.find_elements(By.XPATH, './/div[@class="a-row a-size-small"]/span')
+
+        if rating_box:
+            rating = rating_box[0].get_attribute('aria-label')
+            rating_num = rating_box[1].get_attribute('aria-label')
+        else:
+            rating, rating_num = 0, 0
+
+        product['rating'] = rating
+        product['rating_Num'] = rating_num
+    except NoSuchElementException:
+        product['rating'] = []
+        product['rating_Num'] = []
+
+    # Find Product URL and Open URL
+    product_url = individual_item.find_element(By.XPATH,
+                                    './/a[@class="a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal"]').get_attribute(
+        "href")
+    product['product_url'] = product_url
+
+    driver_product = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=webdriver_options())
+    driver_product.get(product_url)
+
+    try:
+        wait(driver_product, 30).until(
+            EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@class, "a-section a-spacing-none")]')))
+    except (NoSuchElementException, TimeoutException) as e:
+        driver_product.refresh()
+
+    # Find and open All Reviews URL for Product
+    try:
+        review_url = driver_product.find_elements(By.XPATH,
+                                                  '//a[@data-hook="see-all-reviews-link-foot" and contains(@class, "a-link-emphasis a-text-bold")]')
+    except (NoSuchElementException, TimeoutException) as e:
+        driver_product.refresh()
+        review_url = driver_product.find_elements(By.XPATH,
+                                                  '//a[@data-hook="see-all-reviews-link-foot" and contains(@class, "a-link-emphasis a-text-bold")]')
+
+    if review_url != []:
+
+        ############
+        print(review_url[0].get_attribute('href'))
+        ############
+
+        product['review_url'] = review_url[0].get_attribute('href')
+        driver_product.get(product['review_url'])
+        try:
+            wait(driver_product, 30).until(
+                EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, "main")]')))
+        except (NoSuchElementException, TimeoutException) as e:
+            try:
+                driver_product.refresh()
+                wait(driver_product, 30).until(
+                    EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, "main")]')))
+            except (NoSuchElementException, TimeoutException) as e:
+                return product
+
+    else:
+        product['review_url'] = []
+        product['reviews'] = []
+        return product
+
+
+    # Extract Reviews for product
+    reviews = []
+    last_review_page = 0
+
+    while last_review_page != 1:
+
+        try:
+            next_page = driver_product.find_element(By.XPATH, '//li[@class="a-last"]//a')
+
+            reviews.append(scrap_product_reviews(driver_product,reviews))
+
+            driver_product.get(next_page.get_attribute('href'))
+
+            # Check if next page is loaded
+            try:
+                wait(driver_product, 30).until(
+                    EC.presence_of_all_elements_located((By.XPATH, '//div[contains(@role, "main")]')))
+            except NoSuchElementException:
+                driver_product.refresh()
+
+            last_review_page = 0
+
+        except NoSuchElementException:
+            last_review_page = 1
+            reviews.append(scrap_product_reviews(driver_product,reviews))
+
+    product['reviews'] = reviews
+
+    # Close the driver product webdriver
+    driver_product.quit()
+
+    return product
+def scrap_product_reviews(web_driver_product_reviews, review_dataset):
+    # Extract review elements
+
+    try:
+        review_elements = web_driver_product_reviews.find_elements(By.XPATH, '//div[@data-hook="review"]')
+    except NoSuchElementException:
+        print("No Reviews Element Found in Page")
+        review={}
+        review_dataset.append(review)
+        return review_dataset
+
+    for review_element in review_elements:
+        review = {}
+
+        # Extract reviewer name
+        try:
+            reviewer_element = review_element.find_element(By.XPATH, './/span[@class="a-profile-name"]')
+            review['reviewer'] = reviewer_element.text.strip()
+        except NoSuchElementException:
+            review['reviewer'] = 'None'
+            pass
+
+        # Extract review rating
+        try:
+            rating_element = review_element.find_element(By.XPATH, './/i[@data-hook="review-star-rating"]')
+            rating = rating_element.find_element(By.XPATH, './/span[@class="a-icon-alt"]')
+            review['rating'] = rating.get_attribute('textContent').strip().split()[0]
+        except NoSuchElementException:
+            review['rating'] = 'None'
+            pass
+
+        # Extract review text
+        try:
+            review_text_element = review_element.find_element(By.XPATH, './/span[@data-hook="review-body"]')
+            review['review_text'] = review_text_element.text.strip()
+        except NoSuchElementException:
+            review['review_text'] = 'None'
+            pass
+
+        # Extract picture
+        try:
+            review_images_element = review_element.find_elements(By.XPATH,
+                                                                 './/div[@class="review-image-tile-section"]//img')
+            review['img'] = [image.get_attribute('src') for image in review_images_element]
+        except NoSuchElementException:
+            review['img'] = 'None'
+            pass
+
+        #Append review to review_dataset
+        review_dataset.append(review)
+
+    return review_dataset
+def save_products(product_name, products_dataset):
+    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
+    file_name = f'{product_name}_amazon_reviews_{current_time}.txt'
+
+    with open(file_name, 'w') as f:
+        serialized_data = jsonpickle.encode(products_dataset)
+        f.write(serialized_data)
+
+keyword = "mascara"
+search_amazon(keyword)
+
+# Problems
+# Convert downloaded image url to 64bit data (ChatGPT)
+# Add Checking Mechanism to compare newly added product to existing product
+# Add Amazon verification using twocaptcha
+# Add hide my ip to chrome
+
+
+
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"72745fe9-87cb-4a98-b913-8182cde0523a\" name=\"Changes\" comment=\"\">\n      <change beforePath=\"$PROJECT_DIR$/.idea/.gitignore\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.idea/Know Project.iml\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.idea/modules.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/modules.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.idea/vcs.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/vcs.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/scripts/amz_v2.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/scripts/amz_v2.py\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectId\" id=\"2Sg3LdbyG5itxamCiGkD2TnmJIU\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">{\n  &quot;keyToString&quot;: {\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;\n  }\n}</component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"72745fe9-87cb-4a98-b913-8182cde0523a\" name=\"Changes\" comment=\"\" />\n      <created>1689554343492</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1689554343492</updated>\n    </task>\n    <servers />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 5fb15959bf4bf8202136e918ac692ea47b8bcb40)
+++ b/.idea/workspace.xml	(date 1689984196753)
@@ -5,16 +5,24 @@
   </component>
   <component name="ChangeListManager">
     <list default="true" id="72745fe9-87cb-4a98-b913-8182cde0523a" name="Changes" comment="">
-      <change beforePath="$PROJECT_DIR$/.idea/.gitignore" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/Know Project.iml" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/modules.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/modules.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/vcs.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/scripts/sephora.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/scripts/Sephora.py" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/scripts/amz.py" beforeDir="false" />
       <change beforePath="$PROJECT_DIR$/scripts/amz_v2.py" beforeDir="false" afterPath="$PROJECT_DIR$/scripts/amz_v2.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/scripts/amz_v2_ori.py" beforeDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
     <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
+  <component name="FileTemplateManagerImpl">
+    <option name="RECENT_TEMPLATES">
+      <list>
+        <option value="Python Script" />
+      </list>
+    </option>
   </component>
   <component name="MarkdownSettingsMigration">
     <option name="stateVersion" value="1" />
